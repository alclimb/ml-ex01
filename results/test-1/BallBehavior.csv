Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
50000,1.4150455,25.1017745302714,-2.1258938,0.3629242819843342,0.3629242819843342,5.127613,0.024813037,0.00028461154,0.19487052,0.004744038,1.0
100000,1.4105321,21.839579524680072,-0.41596422,0.4901781635449977,0.4901781635449977,0.8544013,0.02494636,0.00025692544,0.1856418,0.004283525,1.0
150000,1.408665,18.92188122758071,0.3316741,0.5888446215139442,0.5888446215139442,0.16451159,0.021438483,0.00022617281,0.17539093,0.0037720068,1.0
200000,1.4079223,18.108521207489492,0.56384164,0.7157050057317539,0.7157050057317539,0.08499251,0.023270696,0.00019541103,0.16513701,0.0032603357,1.0
250000,1.4036348,16.625308424391964,0.6997685,0.8188865398167724,0.8188865398167724,0.0750552,0.025650406,0.00016462822,0.15487604,0.0027483143,1.0
300000,1.3989905,15.184261658031089,0.8340297,0.9332901554404145,0.9332901554404145,0.05932504,0.023017231,0.00013385926,0.14461973,0.0022365241,1.0
350000,1.3992273,14.65393047290949,0.8901787,0.9812147777082029,0.9812147777082029,0.035072707,0.025225386,0.000103115824,0.1343719,0.0017251589,1.0
400000,1.3992703,13.726819248826292,0.9059674,0.9899882214369847,0.9899882214369847,0.02778111,0.02590614,7.236416e-05,0.12412135,0.0012136556,1.0
450000,1.3989564,13.07318451539983,0.9251052,0.9954954954954955,0.9954954954954955,0.026253805,0.025365008,4.4696797e-05,0.1148989,0.0007534551,1.0
500000,1.3986015,12.751650165016502,0.92351145,0.9955995599559956,0.9955995599559956,0.024886543,0.022467438,1.7020418e-05,0.10567343,0.00029310462,1.0
