Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
50000,1.4193511,45.18484288354898,4.255106,-1.5365401413653539,-1.5365401413653539,9.764694,0.022042766,0.0002845985,0.19486615,0.004743821,1.0
100000,1.4178001,57.48795180722892,2.8680997,-1.1392796620601366,-1.1392796620601366,4.187853,0.025471175,0.00025687477,0.18562493,0.004282683,1.0
150000,1.4179327,110.1258581235698,1.8527312,-0.1076430406003713,-0.1076430406003713,1.0387108,0.025355965,0.00022605414,0.17535138,0.003770033,1.0
200000,1.4132669,122.07276995305165,0.889126,2.5331291245011722,2.5331291245011722,0.28221104,0.02437735,0.0001952619,0.16508728,0.0032578553,1.0
250000,1.407107,52.54845580404686,1.1044753,3.4504676940791468,3.4504676940791468,0.22569522,0.02495461,0.00016444373,0.15481457,0.0027452467,1.0
300000,1.399271,44.88929551692589,1.5533545,3.726812757224172,3.726812757224172,0.15553209,0.022813192,0.00013361455,0.14453816,0.0022324543,1.0
350000,1.3931687,42.96478873239437,1.7660042,3.883640689737669,3.883640689737669,0.11021058,0.02278863,0.00010282399,0.13427463,0.0017203044,1.0
400000,1.3868288,47.955882352941174,1.8633977,4.27445046986435,4.27445046986435,0.084422186,0.023644976,7.5110336e-05,0.12503675,0.0012593339,1.0
450000,1.3822712,54.16814159292036,1.9264884,4.5620790922734065,4.5620790922734065,0.09087669,0.022374418,4.736132e-05,0.11578709,0.0007977753,1.0
500000,1.3796338,57.68933177022274,1.973523,4.7727544057201525,4.7727544057201525,0.086386494,0.02120178,1.653189e-05,0.10551059,0.00028497886,1.0
